{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Storing and Organizing Array Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### A little excursion on how to store, organize and handle large array data sets  ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### So far we have used:\n",
    "***NumPy*** files: binary arrays\n",
    "* [save binary: numpy.save()](https://numpy.org/doc/stable/reference/generated/numpy.save.html)\n",
    "* [save multiple arrays: numpy.savez()](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)\n",
    "* [save multiple compressed: numpy.savez.compressed()](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d1 = np.random.random(size = (1000,20))\n",
    "d2 = np.random.random(size = (1000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to binary \n",
    "np.save(\"myArray.npy\", d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "d3=np.load(\"myArray.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "(d1==d3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The HDF5 Data Container Format\n",
    "<img src=\"IMG/HDF_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data with APIs for many programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HDF5 Structure\n",
    "<img src=\"IMG/hdf5-folder.png\" width=800>\n",
    "<font size=5>[Image Source: https://www.sphenisc.com/doku.php/software/development/hdf5-phdf5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HDF5 Key Features:\n",
    "* POSIX-like syntax for internal data structures /path/to/resource\n",
    "    * folders\n",
    "    * meta data\n",
    "    * comments (even code)\n",
    "    * arrays \n",
    "* fast $n$-D data access \n",
    "* data compression\n",
    "* APIs for many programming languages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Python:\n",
    "* ***h5py***: http://docs.h5py.org/en/stable/index.html\n",
    "* ***HDF5 Docs:*** https://portal.hdfgroup.org/display/support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py #this is the HDF5 lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create some random data\n",
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# write it to the same file - in two different arrays\n",
    "with h5py.File('hdf5_data.h5', 'w') as hdf: #note the write mode 'w'\n",
    "    hdf.create_dataset('dataset1', data=matrix1)\n",
    "    hdf.create_dataset('dataset2', data=matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#opening, listing and reading files\n",
    "with h5py.File('hdf5_data.h5','r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('List of datasets in this file: \\n', ls)\n",
    "    data = hdf.get('dataset2') #here data is still some hdf5 object\n",
    "    dataset1 = np.array(data) #need to convert it into numpy\n",
    "    print('Shape of dataset1: \\n', dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "ls = list(f.keys())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Slicing\n",
    "HDF5 support fancy array slicing - so we do not read all data just to get a slice: http://docs.h5py.org/en/latest/high/dataset.html#fancy-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "f['dataset1'][100:120,:] # this notation mostly follows numpy notation -> try different slices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Groups\n",
    "We can organize data in groups, just like in file systems where we have files (here datasets) in folders (here groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1)\n",
    "    G1.create_dataset('dataset4', data = matrix4)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups.h5','r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory:', base_items)\n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in Group2:', G2_items)\n",
    "    G21 = G2.get('/Group2/SubGroup1')\n",
    "    G21_items = list(G21.items())\n",
    "    print('Items in Group21:', G21_items)\n",
    "    dataset3 = np.array(G21.get('dataset3'))\n",
    "    print(dataset3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is happening? Interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Data\n",
    "HDF5 also support native data compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups_compressed.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1, compression=\"gzip\", compression_opts=9)\n",
    "    G1.create_dataset('dataset4', data = matrix4, compression=\"gzip\", compression_opts=9)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3, compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2, compression=\"gzip\", compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "We can add meta information in form of attributes of files, groups and datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'w')\n",
    "\n",
    "# Create the datasets\n",
    "dataset1 = hdf.create_dataset('dataset1', data=matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data=matrix2)\n",
    "\n",
    "# Set attributes\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = '1.1'\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'r')\n",
    "ls = list(hdf.keys())\n",
    "print('List of datasets in this file: \\n', ls)\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "print('Shape of dataset1: \\n', dataset1.shape)\n",
    "#read the attributes\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "print(k[0])\n",
    "print(v[0])\n",
    "print(data.attrs[k[0]])\n",
    "\n",
    "hdf.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "enable_chalkboard": true,
   "footer": "Janis Keuper - SS21",
   "header": "Data Science: Block 4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
