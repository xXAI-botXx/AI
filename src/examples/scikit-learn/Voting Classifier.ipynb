{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung Machine Learning 1: Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lernziel: Schreiben eines eigenen, sklearn-kompatiblen Klassifikators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweis des Studenten: Die Tests wurden etwas 'verschönert'. In die Funktionsweise wurde jedoch nicht eingegriffen. Anstatt die doch recht unansehnlichen Fehlermeldungen anzuzeigen, habe ich mit try und catch die Tests um ein Detail verschönert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereitung: Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./DATA/heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereitung: Trainieren unterschiedlicher Klassifikatoren auf den Daten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and training data\n",
    "y = data['target']\n",
    "X = data.drop('target', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "clf1 = GaussianNB()\n",
    "\n",
    "# Decision Tree\n",
    "clf2 = DecisionTreeClassifier()\n",
    "\n",
    "# KNN Classifier\n",
    "clf3 = KNeighborsClassifier()\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "clf4 = GradientBoostingClassifier()\n",
    "\n",
    "# Random Forest Classifier\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Because scaling is important for some of the classifiers, we have to do it\n",
    "# (It doesn't hurt if we do it for all of them)\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(clf, label, X_train, y_train): \n",
    "    print(label)\n",
    "    print(\"\\nCROSS VALIDATION RESULTS\")\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = \"accuracy\", cv = 10)\n",
    "    print(\"Accuracy:\", np.round(scores,2))\n",
    "    print(\"Mean:\", np.round(scores.mean(),2))\n",
    "    print(\"Standard deviation:\", np.round(scores.std(),2))\n",
    "\n",
    "def evaluation(clf, X_train, y_train, X_test, y_test):     \n",
    "    print(\"\\nACCURACY ON TEST DATA\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(np.round(metrics.accuracy_score(y_test, pred),2))\n",
    "    print(\"\\n-------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.88 0.72 0.75 0.67 0.96 0.88 0.83 0.79 0.79 0.83]\n",
      "Mean: 0.81\n",
      "Standard deviation: 0.08\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n",
      "Decision Tree\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.8  0.56 0.79 0.71 0.79 0.83 0.88 0.83 0.54 0.83]\n",
      "Mean: 0.76\n",
      "Standard deviation: 0.11\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.75\n",
      "\n",
      "-------------------\n",
      "\n",
      "kNN\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.8  0.76 0.88 0.58 0.92 0.88 0.83 0.88 0.71 0.88]\n",
      "Mean: 0.81\n",
      "Standard deviation: 0.1\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n",
      "GradientBoosting\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.88 0.64 0.71 0.75 0.88 0.79 0.83 0.88 0.71 0.92]\n",
      "Mean: 0.8\n",
      "Standard deviation: 0.09\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.77\n",
      "\n",
      "-------------------\n",
      "\n",
      "RandomForest\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.96 0.64 0.83 0.75 0.92 0.83 0.83 0.88 0.75 0.88]\n",
      "Mean: 0.83\n",
      "Standard deviation: 0.09\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.84\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [('Naive Bayes', clf1), ('Decision Tree', clf2), ('kNN', clf3), ('GradientBoosting', clf4), ('RandomForest', clf5)]\n",
    "\n",
    "for label, clf in classifiers: \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', std_scaler), \n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    cross_validate(pipe, label, X_train, y_train)\n",
    "    evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Implementierung eines Maximum Voting Classifiers (17 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie einen Voting Classifier. Bitte beachten Sie dabei folgendes: \n",
    "* Verwenden Sie für Ihre Implementierung das untenstehende Template. \n",
    "* Ihr Klassifikator soll scikit-learn kompatibel sein. Beachten Sie daher die Hinweise im beigefügten PDF. (Detailliertere Informationen finden Sie hier: https://scikit-learn.org/stable/developers/develop.html, wobei diese Dokumentation viel weiter in die Tiefe geht, als wir es für die Übung brauchen.) Sie finden außerdem ganz unten in diesem Notebook ein Beispiel für eine Implementierung eines Classifiers in diesem Template.  \n",
    "* Eine Liste mit den zu verwendeten Klassifikatoren soll dem VotingClassifier bei der Initialisierung übergeben werden.\n",
    "* Es soll möglich sein beim Aufruf zwischen einem Soft Voting und Hard Voting zu wählen (auch dies wird bei der Initialisierung festgelegt, wobei der entsprechende Parameter die beiden Ausprägungen \"hard\" oder \"soft\" annehmen kann.)\n",
    "\n",
    "\n",
    "Der Klassifikator soll robust sein, d.h. mit einigen möglichen Fehlerfällen umgehen können. Dazu gehört unter anderem: \n",
    "* Wird ein anderer Wert als \"soft\" oder \"hard\" übergeben, soll ein ``ValueError`` geworfen werden mit dem Text: \"voting must be 'hard' or 'soft'; got X\" (wobei X der übergebene Wert ist).\n",
    "* Wenn die Liste der Klassifikatoren = None ist oder sie leer ist, soll ebenfalls ein ``ValueError`` geworfen werden. Dieses Mal mit dem Text \"At least 1 classifier must be submitted\".\n",
    "* Die Methode predict() darf nicht aufgerufen werden, wenn nicht zuvor fit() aufgerufen worden ist. Dies muss in der predict()-Methode mit einer für Ihren Classifier geeignete Methode überprüft werden und ggf. ein ``NotFittedError`` geworfen werden.\n",
    "* Es muss sichergestellt werden, dass X und y in fit() die gleiche Anzahl Instanzen haben. Ist dies nicht der Fall, so soll ein ``ValueError`` geworfen werden.\n",
    "\n",
    "\n",
    "Tipp: Gehen Sie bei der Bearbeitung der Aufgabe folgendermaßen vor: \n",
    "* Stellen Sie zunächst sicher, dass Sie wissen, wie ein VotingClassifier funktioniert (siehe Vorlesungsfolien)\n",
    "* Überlegen Sie anschließend, welche Schritte in welchem Teil des Templates durchgeführt werden müssen (notieren Sie sich diese ggf. kurz in Kommentaren an der entsprechenden Stelle im Code-Template)\n",
    "* Die für die Robustheit erforderlichen Punkte können in einem 2. Schritt bearbeitet werden. D.h. es ist möglich, dass Sie zunächst die Grundfunktionalität des Codes implementieren und testen. Test 1 und 2 unten sollten anschließend laufen. \n",
    "* Führen Sie dann die für die Robustheit erforderlichen Erweiterungen durch und testen Sie diese mit Test 3-7 unten. Tipp: Hier können Sie zum Teil von scikit-learn bereitgestellte Testmethoden verwenden. Beachten Sie hierzu die Hinweise im beigefügten PDF und informieren Sie sich über die entsprechenden Methoden. Überlegen Sie sich jeweils, wo (in welcher Methode) sie die Validierungen jeweils vornehmen müssen. Ggf. müssen die Methoden auch anders parametrisiert werden, als in den Beispielen zu sehen. \n",
    "* Machen Sie anschließend weiter mit Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin): \n",
    "    \n",
    "    \n",
    "    def __init__(self, classifiers = [DecisionTreeClassifier()], voting='hard'):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"        \n",
    "        self.classifiers = classifiers\n",
    "        self.voting = voting  \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.  \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            shape = [n_examples, n_features]\n",
    "            Matrix of training examples.\n",
    "        \n",
    "        y : array-like, shape = [n_examples]\n",
    "            Vector of target class labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        # -> classifier und voting prüfen\n",
    "        X, y = check_X_y(X, y)\n",
    "        # -> x,y gleiche anzahl\n",
    "        if self.voting not in ['hard', 'soft']:\n",
    "            raise ValueError(\"voting must be 'hard' or 'soft'; got \"+str(self.voting))\n",
    "            \n",
    "        if self.classifiers == None or len(self.classifiers) < 1:\n",
    "            raise ValueError(\"At least 1 classifier must be submitted\")\n",
    "            \n",
    "        for model in self.classifiers:\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "            except:\n",
    "                raise ValueError(\"There is something wrong with the classifiers!\")\n",
    "            \n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # nicht nur für 2 Klassen programmiert worden :)\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            Shape = [n_examples, n_features]\n",
    "            Matrix of training examples.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_examples]\n",
    "            Predicted class labels.\n",
    "        \n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "        result = []\n",
    "        predictions = []\n",
    "        if self.voting == 'hard':\n",
    "            # Funktionsweise:\n",
    "            # 1. Lass die Classifier ihre Vorhersage treffen\n",
    "            # 2. Summiere die einzelnen Vorhersagen von jedem Classifier(für jeden Datenpunkt)\n",
    "            # 3. Finde die Klasse, die insgesamt am öftesten Vorhergesagt wurde (wieder für jeden Datenpunkt)\n",
    "            \n",
    "            # Hinweis: der Code wird durch 2 Faktoren verkompliziert:\n",
    "            #    - Zum Einen gibt es mehrere Datenpunkte. Man muss also insgesamt mehrere Objekte klassifizieren.\n",
    "            #      Im Code muss man deswegen alle Spalten durchgehen. Jede Spalte stellt in diesem Fall einen \n",
    "            #      Datenpunkt da. Außerdem muss man die Ergebnisse für jeden Datenpunkt getrennt abspeichern. \n",
    "            #      Warum? Naja die Vorhersagen gelten natürlich immer nur für einen Datenpunkt.\n",
    "            #      Die Resultate einer Vorhersage muss man deswegen für jeden Datenpunkt einzeln abspeichern.\n",
    "            #      Und das kann man in einer Liste am besten realisieren.\n",
    "            #    - Zum Anderen habe ich den Code für eine beliebige Anzahl von Klassen geschrieben.\n",
    "            #      Mein Code ist deshalb etwas abstrakter gehalten, aber eigentlich auch nicht viel komplizierter.\n",
    "            \n",
    "            # 1. Lass die Classifier ihre Vorhersage treffen\n",
    "            for model in self.classifiers:\n",
    "                result += [model.predict(X).tolist()]\n",
    "                \n",
    "            # 2. Summiere die einzelnen Vorhersagen von jedem Classifier(für jeden Datenpunkt)\n",
    "            for column in range(len(result[0])):\n",
    "                for row in range(len(result)):\n",
    "                    # sort result\n",
    "                    voting = dict()\n",
    "                    vote = result[row][column]\n",
    "                    if voting.get(vote) == None:\n",
    "                        voting[vote] = 1\n",
    "                    else:\n",
    "                        voting[vote] += 1\n",
    "                        \n",
    "                # 3. Finde die Klasse, die insgesamt am öftesten Vorhergesagt wurde (wieder für jeden Datenpunkt)\n",
    "                pred = None\n",
    "                for v in voting.items():\n",
    "                    if pred == None:\n",
    "                        pred = v[0]\n",
    "                    elif v[1] > voting[pred]:\n",
    "                        pred = v[0]\n",
    "                predictions += [pred]\n",
    "            return predictions\n",
    "        \n",
    "        elif self.voting == 'soft':\n",
    "            # Funktionsweise:\n",
    "            # 1. Lass die Classifier ihre vorhersage treffen (als Wahrscheinlichkeit)\n",
    "            # 2. Summiere die einzelnen Wahrscheinlichkeiten (für jeden Datenpunkt)\n",
    "            # 3. Finde die Klasse, die insgesamt die höchste Wahrscheinlichkeit hat (wieder für jeden Datenpunkt)\n",
    "            \n",
    "            # (wie auch oben)\n",
    "            # Hinweis: der Code wird durch 2 Faktoren verkompliziert:\n",
    "            #    - Zum Einen gibt es mehrere Datenpunkte. Man muss also insgesamt mehrere Objekte klassifizieren.\n",
    "            #      Im Code muss man deswegen alle Spalten durchgehen. Jede Spalte stellt in diesem Fall einen \n",
    "            #      Datenpunkt da. Außerdem muss man die Ergebnisse für jeden Datenpunkt getrennt abspeichern. \n",
    "            #      Warum? Naja die Vorhersagen gelten natürlich immer nur für einen Datenpunkt.\n",
    "            #      Die Resultate einer Vorhersage muss man deswegen für jeden Datenpunkt einzeln abspeichern.\n",
    "            #      Und das kann man in einer Liste am besten realisieren.\n",
    "            #    - Zum Anderen habe ich den Code für eine beliebige Anzahl von Klassen geschrieben.\n",
    "            #      Mein Code ist deshalb etwas abstrakter gehalten, aber eigentlich auch nicht viel komplizierter.\n",
    "            \n",
    "            # 1. Lass die Classifier ihre vorhersage treffen (als Wahrscheinlichkeit)\n",
    "            for model in self.classifiers:\n",
    "                result += [model.predict_proba(X).tolist()]\n",
    "                \n",
    "            # 2. Summiere die einzelnen Wahrscheinlichkeiten (für jeden Datenpunkt)\n",
    "            pred_sum_collection = []\n",
    "            for column in range(len(result[0])):\n",
    "                pred_sum = dict()\n",
    "                for row in range(len(result)):\n",
    "                    for class_pos in range(len(result[row][column])):\n",
    "                        if pred_sum.get(class_pos) == None:\n",
    "                            pred_sum[class_pos] = result[row][column][class_pos]\n",
    "                        else:\n",
    "                            pred_sum[class_pos] += result[row][column][class_pos]\n",
    "                pred_sum_collection += [pred_sum]\n",
    "                \n",
    "            # 3. Finde die Klasse, die insgesamt die höchste Wahrscheinlichkeit hat (wieder für jeden Datenpunkt)\n",
    "            pred_collection = []\n",
    "            # Gehe die summierten Wahrscheinlichkeiten der einzelnen Datenpunkte durch\n",
    "            for column in pred_sum_collection:\n",
    "                # Berechne die zu vorhersagende Klasse für diesen Datenpunkt\n",
    "                pred = None\n",
    "                for p in column.items():\n",
    "                    if pred == None:\n",
    "                        pred = p[0]\n",
    "                    elif p[1] > column[pred]:\n",
    "                        pred = p[0]\n",
    "                # Füge die nun gefundene wahrscheinlichste Klasse zu den Ergebnissen\n",
    "                pred_collection += [pred]\n",
    "            return pred_collection\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n",
    "Wenden Sie Ihren MajorityVoteClassifier mit voting = \"hard\" an. Führen Sie eine Cross-Validation durch und bestimmen Sie außerdem die Accuracy auf den Testdaten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.96 0.72 0.88 0.75 0.96 0.79 0.79 0.88 0.79 0.88]\n",
      "Mean: 0.84\n",
      "Standard deviation: 0.08\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.84\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble = [clf1, clf2, clf3, clf4, clf5]\n",
    "my_clf = MajorityVoteClassifier(ensemble, voting = \"hard\")\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)\n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n",
    "Ändern Sie den Parameter Voting auf \"soft\". Führen Sie erneut eine Cross-Validation durch und bestimmen Sie außerdem die Accuracy auf den Testdaten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.88 0.68 0.83 0.75 0.92 0.79 0.88 0.83 0.71 0.88]\n",
      "Mean: 0.81\n",
      "Standard deviation: 0.08\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.82\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_clf_soft = MajorityVoteClassifier(ensemble, voting = \"soft\")\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf_soft)    \n",
    "])\n",
    "\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n",
    "Ihr MajorityVotingClassifier sollte auch mit der Situation umgehen können, dass nur 1 Klassifikator übergeben wird. Stellen Sie sicher, dass Ihr Voting Classifier auch dann funktionsfähig ist und demonstrieren Sie dies hier durch den untenstehenden Aufruf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.88 0.72 0.75 0.67 0.96 0.88 0.83 0.79 0.79 0.83]\n",
      "Mean: 0.81\n",
      "Standard deviation: 0.08\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_1 = [clf1]\n",
    "my_clf = MajorityVoteClassifier(ensemble_1, voting = \"hard\")\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)    \n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4\n",
    "Ihr MajorityVotingClassifier sollte auch mit der Situation umgehen können, dass eine leere Liste von Klassifikatoren übergeben wird und in diesem Fall die oben definierte Fehlermeldung werfen. Stellen Sie sicher, dass dies der Fall ist und demonstrieren Sie hier die Funktionsweise durch einen Aufruf mit einer leeren Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr gut, Test bestanden! :D\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ensemble_0 = []\n",
    "    my_clf = MajorityVoteClassifier(ensemble_0, voting = \"hard\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', std_scaler), \n",
    "        ('classifier', my_clf)    \n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe.predict(X_test)\n",
    "    print(\"Es wurde leider keine Exception geworfen. Test nicht bestanden!\")\n",
    "except ValueError:\n",
    "    print(\"Sehr gut, Test bestanden! :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5\n",
    "Wird ``predict()`` vor ``fit()`` aufgerufen, so soll ein ``NotFittedError`` geworfen werden.  \n",
    "Stellen Sie sicher, dass dies der Fall ist und demonstrieren Sie hier die Funktionsweise durch einen Aufruf von ``predict()`` ohne vorherigen Aufruf von ``fit()``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr gut, Test bestanden! :D\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_clf2 = MajorityVoteClassifier(ensemble, voting = \"hard\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', std_scaler), \n",
    "        ('classifier', my_clf2)\n",
    "    ])\n",
    "\n",
    "    pipe.predict(X_test)\n",
    "    print(\"Es wurde leider keine Exception geworfen. Test nicht bestanden!\")\n",
    "except NotFittedError:\n",
    "    print(\"Sehr gut, Test bestanden! :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6\n",
    "Wenn die Anzahl Zeilen in X nicht mit der Anzahl der Zeilen in y übereinstimmt, soll ein ``ValueError`` geworfen werden. Stellen Sie sicher, dass dies der Fall ist und demonstrieren Sie hier die Funktionsweise durch den unten stehenden Aufruf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr gut, Test bestanden! :D\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_clf = MajorityVoteClassifier(ensemble, voting = \"hard\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', std_scaler), \n",
    "        ('classifier', my_clf)\n",
    "    ])\n",
    "    X_train2 = X_train.iloc[3:,:]\n",
    "\n",
    "    pipe.fit(X_train2, y_train)\n",
    "    pipe.predict(X_test)\n",
    "    print(\"Es wurde leider keine Exception geworfen. Test nicht bestanden!\")\n",
    "except ValueError:\n",
    "    print(\"Sehr gut, Test bestanden! :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7\n",
    "Wenn ein anderer String als \"hard\" oder \"soft\" übergeben wird, soll eine Fehlermeldung geworfen werden. Demonstrieren Sie durch Aufruf des unten stehenden Codes, dass das bei Ihnen der Fall ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr gut, Test bestanden! :D\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_clf = MajorityVoteClassifier(ensemble, voting = \"hard2\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', std_scaler), \n",
    "        ('classifier', my_clf)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe.predict(X_test)\n",
    "    print(\"Es wurde leider keine Exception geworfen. Test nicht bestanden!\")\n",
    "except ValueError:\n",
    "    print(\"Sehr gut, Test bestanden! :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 (3 Punkte)\n",
    "Entfernen Sie den in der Einzelwertung schwächsten Klassifikator (bzw. ggf. auch 2 Klassifikatoren, falls es mehrere gibt, die deutlich schlechter als alle anderen sind) und testen Sie den Voting Classifier erneut. Führen Sie den Test sowohl mit voting = \"soft\" als auch voting = \"hard\" durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurung kommt weiter unten\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import metrics\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin): \n",
    "    \n",
    "    \n",
    "    def __init__(self, classifiers = [DecisionTreeClassifier()], voting='hard'):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"        \n",
    "        self.classifiers = classifiers\n",
    "        self.voting = voting  \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.  \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            shape = [n_examples, n_features]\n",
    "            Matrix of training examples.\n",
    "        \n",
    "        y : array-like, shape = [n_examples]\n",
    "            Vector of target class labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        # -> classifier und voting prüfen\n",
    "        X, y = check_X_y(X, y)\n",
    "        # -> x,y gleiche anzahl\n",
    "        if self.voting not in ['hard', 'soft']:\n",
    "            raise ValueError(\"voting must be 'hard' or 'soft'; got \"+str(self.voting))\n",
    "            \n",
    "        if self.classifiers == None or len(self.classifiers) < 1:\n",
    "            raise ValueError(\"The model needs classifiers to work!\")\n",
    "            \n",
    "        for model in self.classifiers:\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "            except:\n",
    "                raise ValueError(\"There is something wrong with the classifiers!\")\n",
    "            \n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # nicht nur für 2 Klassen programmiert worden :)\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix},\n",
    "            Shape = [n_examples, n_features]\n",
    "            Matrix of training examples.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_examples]\n",
    "            Predicted class labels.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Entferne schwache Classifier:\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "        result = []\n",
    "        predictions = []\n",
    "        if self.voting == 'hard':\n",
    "            # Funktionsweise:\n",
    "            # 1. Lass die Classifier ihre Vorhersage treffen\n",
    "            # 2. Summiere die einzelnen Vorhersagen von jedem Classifier(für jeden Datenpunkt)\n",
    "            # 3. Finde die Klasse, die insgesamt am öftesten Vorhergesagt wurde (wieder für jeden Datenpunkt)\n",
    "            \n",
    "            # Hinweis: der Code wird durch 2 Faktoren verkompliziert:\n",
    "            #    - Zum Einen gibt es mehrere Datenpunkte. Man muss also insgesamt mehrere Objekte klassifizieren.\n",
    "            #      Im Code muss man deswegen alle Spalten durchgehen. Jede Spalte stellt in diesem Fall einen \n",
    "            #      Datenpunkt da. Außerdem muss man die Ergebnisse für jeden Datenpunkt getrennt abspeichern. \n",
    "            #      Warum? Naja die Vorhersagen gelten natürlich immer nur für einen Datenpunkt.\n",
    "            #      Die Resultate einer Vorhersage muss man deswegen für jeden Datenpunkt einzeln abspeichern.\n",
    "            #      Und das kann man in einer Liste am besten realisieren.\n",
    "            #    - Zum Anderen habe ich den Code für eine beliebige Anzahl von Klassen geschrieben.\n",
    "            #      Mein Code ist deshalb etwas abstrakter gehalten, aber eigentlich auch nicht viel komplizierter.\n",
    "            \n",
    "            # 1. Lass die Classifier ihre Vorhersage treffen\n",
    "            for model in self.classifiers:\n",
    "                result += [model.predict(X).tolist()]\n",
    "                \n",
    "            # 2. Summiere die einzelnen Vorhersagen von jedem Classifier(für jeden Datenpunkt)\n",
    "            for column in range(len(result[0])):\n",
    "                for row in range(len(result)):\n",
    "                    # sort result\n",
    "                    voting = dict()\n",
    "                    vote = result[row][column]\n",
    "                    if voting.get(vote) == None:\n",
    "                        voting[vote] = 1\n",
    "                    else:\n",
    "                        voting[vote] += 1\n",
    "                        \n",
    "                # 3. Finde die Klasse, die insgesamt am öftesten Vorhergesagt wurde (wieder für jeden Datenpunkt)\n",
    "                pred = None\n",
    "                for v in voting.items():\n",
    "                    if pred == None:\n",
    "                        pred = v[0]\n",
    "                    elif v[1] > voting[pred]:\n",
    "                        pred = v[0]\n",
    "                predictions += [pred]\n",
    "            return predictions\n",
    "        \n",
    "        elif self.voting == 'soft':\n",
    "            # Funktionsweise:\n",
    "            # 1. Lass die Classifier ihre vorhersage treffen (als Wahrscheinlichkeit)\n",
    "            # 2. Summiere die einzelnen Wahrscheinlichkeiten (für jeden Datenpunkt)\n",
    "            # 3. Finde die Klasse, die insgesamt die höchste Wahrscheinlichkeit hat (wieder für jeden Datenpunkt)\n",
    "            \n",
    "            # (wie auch oben)\n",
    "            # Hinweis: der Code wird durch 2 Faktoren verkompliziert:\n",
    "            #    - Zum Einen gibt es mehrere Datenpunkte. Man muss also insgesamt mehrere Objekte klassifizieren.\n",
    "            #      Im Code muss man deswegen alle Spalten durchgehen. Jede Spalte stellt in diesem Fall einen \n",
    "            #      Datenpunkt da. Außerdem muss man die Ergebnisse für jeden Datenpunkt getrennt abspeichern. \n",
    "            #      Warum? Naja die Vorhersagen gelten natürlich immer nur für einen Datenpunkt.\n",
    "            #      Die Resultate einer Vorhersage muss man deswegen für jeden Datenpunkt einzeln abspeichern.\n",
    "            #      Und das kann man in einer Liste am besten realisieren.\n",
    "            #    - Zum Anderen habe ich den Code für eine beliebige Anzahl von Klassen geschrieben.\n",
    "            #      Mein Code ist deshalb etwas abstrakter gehalten, aber eigentlich auch nicht viel komplizierter.\n",
    "            \n",
    "            # 1. Lass die Classifier ihre vorhersage treffen (als Wahrscheinlichkeit)\n",
    "            for model in self.classifiers:\n",
    "                result += [model.predict_proba(X).tolist()]\n",
    "                \n",
    "            # 2. Summiere die einzelnen Wahrscheinlichkeiten (für jeden Datenpunkt)\n",
    "            pred_sum_collection = []\n",
    "            for column in range(len(result[0])):\n",
    "                pred_sum = dict()\n",
    "                for row in range(len(result)):\n",
    "                    for class_pos in range(len(result[row][column])):\n",
    "                        if pred_sum.get(class_pos) == None:\n",
    "                            pred_sum[class_pos] = result[row][column][class_pos]\n",
    "                        else:\n",
    "                            pred_sum[class_pos] += result[row][column][class_pos]\n",
    "                pred_sum_collection += [pred_sum]\n",
    "                \n",
    "            # Entferne \n",
    "                \n",
    "            # 3. Finde die Klasse, die insgesamt die höchste Wahrscheinlichkeit hat (wieder für jeden Datenpunkt)\n",
    "            pred_collection = []\n",
    "            # Gehe die summierten Wahrscheinlichkeiten der einzelnen Datenpunkte durch\n",
    "            for column in pred_sum_collection:\n",
    "                # Berechne die zu vorhersagende Klasse für diesen Datenpunkt\n",
    "                pred = None\n",
    "                for p in column.items():\n",
    "                    if pred == None:\n",
    "                        pred = p[0]\n",
    "                    elif p[1] > column[pred]:\n",
    "                        pred = p[0]\n",
    "                # Füge die nun gefundene wahrscheinlichste Klasse zu den Ergebnissen\n",
    "                pred_collection += [pred]\n",
    "            return pred_collection\n",
    "        \n",
    "        \n",
    "    # Neu hinzugefügt!!! (Aufgabe 2)\n",
    "    \n",
    "    # Helper:\n",
    "    # berechne die Accuracy für einen Classifier -> wird in remove_weak_classifier verwendet\n",
    "    def calc_accuracy(self, clf, X_train, y_train, X_test, y_test):  \n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        return np.round(metrics.accuracy_score(y_test, pred),2)\n",
    "\n",
    "    # Helper:\n",
    "    # berechnet den Median von einer Liste -> ist speziell für die Accuracy angepasst, welche ich so gespeichert habe: (index, accuracy)\n",
    "    #                                                                                      -> siehe bei remove_weak_classifier\n",
    "    def calc_median(self, x:list):\n",
    "        if len(x) > 1:\n",
    "            if len(x)%2 == 0:\n",
    "                half1 = len(x)//2\n",
    "                half2= len(x)//2 -1\n",
    "                return (x[half1][1] + x[half2][1])/2\n",
    "            else:\n",
    "                half = len(x)//2 + 1\n",
    "                return x[half][1]\n",
    "\n",
    "    # Entfernt schwache Classifier\n",
    "    # wird vor predict und vor fit aufgerufen -> siehe die Tests weiter unten\n",
    "    def remove_weak_classifier(self, X_train, y_train, X_test, y_test):\n",
    "        # Berechne die Accuracy von jedem classifier\n",
    "        accuracy = []\n",
    "        counter = 0\n",
    "        for classifier in self.classifiers:\n",
    "            accuracy += [(counter, self.calc_accuracy(classifier, X_train, y_train, X_test, y_test))]\n",
    "            #                       -> Ich speicher mir noch den Index von dem Classifier ab, damit ich noch weis, zu wem die Accuracy gehört\n",
    "            #                          Ich sortiere nämlich gleich die Liste\n",
    "            counter += 1\n",
    "\n",
    "        # Brechne den Median -> damit man Abweichungen feststellen kann\n",
    "        # muss man nicht machen, bzw. man könnte es auch anders lösen\n",
    "        median = self.calc_median(accuracy)\n",
    "        \n",
    "        # Schwächste Accuracies ausfindig machen -> vorne die schwächsten und nach hinten immer stärker\n",
    "        # lambda x:x[1], da accuracy eine Liste von Tupel ist [(index, accuracy), ()...]\n",
    "        # und wir wollen hier nach der accuracy sortieren: die liegt auf dem index 1, wenn man ein Element anschaut\n",
    "        accuracy.sort(key=lambda x:x[1], reverse=False)\n",
    "        \n",
    "        # Je nach Länge: prüfe ob kleiner als Median -> man hätte sie auch einfach direkt ohne überprüfung entfernen können\n",
    "        to_remove = []\n",
    "        if len(accuracy) > 1 and len(accuracy) <= 4:\n",
    "            if accuracy[0][1] < median:\n",
    "                    to_remove += [accuracy[0][0]]\n",
    "        elif len(accuracy) >= 5:\n",
    "            for acc in range(2):\n",
    "                # Man hätte auch direkt die letzten beiden entfernen können\n",
    "                # oder eine anderes Ausschusskriterium wählen können (nicht unbedingt median)\n",
    "                if accuracy[acc][1] < median:\n",
    "                    to_remove += [accuracy[acc][0]]\n",
    "                    \n",
    "        # Entferne nun die gerade gefundenen schwachen Classifier             \n",
    "        new_classifiers = []\n",
    "        for i, x in enumerate(self.classifiers):\n",
    "            if i in to_remove:\n",
    "                # ne, den wollen wir nicht in unsere Liste aufnehmen\n",
    "                continue\n",
    "            new_classifiers += [x]\n",
    "        print(\"\\n-----\\nOld_Classifiers\", self.classifiers)\n",
    "        print(\"New_Classifiers\", new_classifiers)\n",
    "        print(\"accuracies:\", accuracy)\n",
    "        print(\"median:\", median,\"\\n-----\\n\")\n",
    "        self.classifiers = new_classifiers\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "Old_Classifiers [GaussianNB(), DecisionTreeClassifier(), KNeighborsClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "New_Classifiers [GaussianNB(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "accuracies: [(2, 0.7), (1, 0.75), (3, 0.79), (4, 0.84), (0, 0.87)]\n",
      "median: 0.79 \n",
      "-----\n",
      "\n",
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.96 0.64 0.88 0.75 0.96 0.83 0.79 0.88 0.79 0.88]\n",
      "Mean: 0.84\n",
      "Standard deviation: 0.09\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.82\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_clf = MajorityVoteClassifier(ensemble, voting = \"hard\")\n",
    "my_clf.remove_weak_classifier(X_train, y_train, X_test, y_test)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)\n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "Old_Classifiers [GaussianNB(), DecisionTreeClassifier(), KNeighborsClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "New_Classifiers [GaussianNB(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "accuracies: [(2, 0.7), (1, 0.72), (3, 0.79), (4, 0.82), (0, 0.87)]\n",
      "median: 0.79 \n",
      "-----\n",
      "\n",
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.92 0.76 0.83 0.79 0.88 0.83 0.79 0.83 0.75 0.88]\n",
      "Mean: 0.83\n",
      "Standard deviation: 0.05\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_clf = MajorityVoteClassifier(ensemble, voting = \"soft\")\n",
    "my_clf.remove_weak_classifier(X_train, y_train, X_test, y_test)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)\n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "Old_Classifiers [GaussianNB()]\n",
      "New_Classifiers [GaussianNB()]\n",
      "accuracies: [(0, 0.87)]\n",
      "median: None \n",
      "-----\n",
      "\n",
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.88 0.72 0.75 0.67 0.96 0.88 0.83 0.79 0.79 0.83]\n",
      "Mean: 0.81\n",
      "Standard deviation: 0.08\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_1 = [clf1]\n",
    "my_clf = MajorityVoteClassifier(ensemble_1, voting = \"hard\")\n",
    "my_clf.remove_weak_classifier(X_train, y_train, X_test, y_test)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)    \n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "Old_Classifiers [GaussianNB(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "New_Classifiers [GaussianNB(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
      "accuracies: [(1, 0.75), (2, 0.77), (3, 0.8), (0, 0.87)]\n",
      "median: 0.76 \n",
      "-----\n",
      "\n",
      "voting_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.92 0.68 0.88 0.75 0.92 0.79 0.79 0.79 0.79 0.88]\n",
      "Mean: 0.82\n",
      "Standard deviation: 0.07\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.82\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble = [clf1, clf2, clf4, clf5]\n",
    "my_clf = MajorityVoteClassifier(ensemble, voting = \"hard\")\n",
    "my_clf.remove_weak_classifier(X_train, y_train, X_test, y_test)\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)\n",
    "])\n",
    "cross_validate(pipe, \"voting_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschlussaufgabe (unbewertet)\n",
    "Vergleichen Sie nun die Ergebnisse aller Tests. Womit konnten Sie die besten Ergebnisse erzielen? Wie stabil und somit belastbar sind die Ergebnisse? Was könnten nächste Schritte sein, um eine weitere Verbesserung zu erreichen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Entfgernung schwacher Classifier zeigt eine leichte Verbesserung. Jedoch schwankt die Accuracy natürlich. <br>\n",
    "Vor allem die Accuracy beim Soft-Voting verbessert sich. <br>\n",
    "Wenn man im Vorhinein nur Klassifizierer, die für die Situation geeigent sind, wählt, würde man den Klassifizierer schon in die richtige Richtung schieben.<br>\n",
    "<br>\n",
    "Als Letztes habe ich bemerkt, dass der VotingClassifier mit weniger Classifiern nicht so stabil ist, wie der mit mehr Classifiern. Das ergibt eigentlich auch Sinn, da das Ergebnis stärker von den einzelnen Ergebnissen abhängig ist (umso weniger Classifier verwendet werden).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offizielles Beispiel-Template für einen scikit-learn kompatiblen Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**----------**<br>**Hinweis um sich nicht von diesem Beispiel verwirren zulassen:**<br><br>\n",
    "Hier wird ein KNN-Klassifizierer realisiert, wobei nur der erste nächste Nachbar zum klassifizieren verwendet wird (k=1).<br>\n",
    "Der KNN muss die Trainingsdaten nicht lernen, sondern speichert sie und bei der Vorhersage, schaut er einfach, welcher der nächste Nachbar ist und gibt die Klasse von diesem Punkt zurück.<br>\n",
    "Man sagt auch lazy-learner zu dem KNN. Und genau deswegen ist in der fit-Funktion nichts wirkliches zu sehen.<br>\n",
    "Die Klassifizieren findet dann bei predict() statt.<br><br>\n",
    "-> closest = np.argmin(euclidean_distances(X, self.X_), axis=1)<br>\n",
    "->        return self.y_[closest]<br>\n",
    "<br>\n",
    "Diese beiden Zeilen finden den nächsten Nachbarn und geben die Klasse von diesem Punkt zurück.<br>\n",
    "**----------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "class TemplateClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\" An example classifier which implements a 1-NN algorithm.\n",
    "    For more information regarding how to build your own classifier, read more\n",
    "    in the :ref:`User Guide <user_guide>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    demo_param : str, default='demo'\n",
    "        A parameter used for demonstation of how to pass and store paramters.\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_ : ndarray, shape (n_samples, n_features)\n",
    "        The input passed during :meth:`fit`.\n",
    "    y_ : ndarray, shape (n_samples,)\n",
    "        The labels passed during :meth:`fit`.\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        The classes seen at :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"A reference implementation of a fitting function for a classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values. An array of int.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" A reference implementation of a prediction for a classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The label for each sample is the label of the closest sample\n",
    "            seen during fit.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_clf\n",
      "\n",
      "CROSS VALIDATION RESULTS\n",
      "Accuracy: [0.76 0.56 0.75 0.62 0.83 0.79 0.79 0.75 0.58 0.83]\n",
      "Mean: 0.73\n",
      "Standard deviation: 0.1\n",
      "\n",
      "ACCURACY ON TEST DATA\n",
      "0.87\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_clf = TemplateClassifier()\n",
    "pipe = Pipeline([\n",
    "    ('scaler', std_scaler), \n",
    "    ('classifier', my_clf)\n",
    "])\n",
    "cross_validate(pipe, \"knn_clf\", X_train, y_train)\n",
    "evaluation(pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
